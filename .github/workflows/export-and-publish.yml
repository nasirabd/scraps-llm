name: Export ONNX & Publish

on:
  workflow_dispatch:
  push:
    branches: [ main ]
    paths:
      - "export/**"
      - "src/**"
      - "configs/**"
      - "tokenizer/**"

jobs:
  export-and-publish:
    runs-on: ubuntu-latest
    timeout-minutes: 30

    env:
      HF_TOKEN:       ${{ secrets.HF_TOKEN }}
      HF_MODEL_REPO:  donribbs/scraps-llm-model
      HF_CKPT_PATH:   best_model.pt  

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          cache: "pip"

      - name: Install dependencies
        run: |
            python -m pip install --upgrade pip
            # Install CPU wheels to keep CI light
            pip install --index-url https://download.pytorch.org/whl/cpu torch==2.4.1
            # ONNX toolchain 
            pip install onnx==1.16.2 onnxscript==0.1.41 onnxruntime==1.19.2
            # Your project deps
            pip install -r requirements.txt || true
            pip install huggingface_hub pyyaml

      - name: Download checkpoint from HF model repo
        env:
          HF_TOKEN:      ${{ env.HF_TOKEN }}
          HF_MODEL_REPO: ${{ env.HF_MODEL_REPO }}
          HF_CKPT_PATH:  ${{ env.HF_CKPT_PATH }}
        run: |
          python - << 'PY'
          import os
          from huggingface_hub import hf_hub_download
          from pathlib import Path

          token    = os.environ["HF_TOKEN"]
          repo_id  = os.environ["HF_MODEL_REPO"]
          ckpt_rel = os.environ.get("HF_CKPT_PATH", "best_model.pt")

          Path("model/checkpoints").mkdir(parents=True, exist_ok=True)
          p = hf_hub_download(
              repo_id=repo_id,
              repo_type="model",
              filename=ckpt_rel,
              local_dir="model/checkpoints",
              local_dir_use_symlinks=False,
              token=token,
          )
          print(f"✅ Downloaded checkpoint: {p}")
          PY
          
      - name: Ensure repo root on PYTHONPATH
        run: echo "PYTHONPATH=${GITHUB_WORKSPACE}" >> $GITHUB_ENV

      - name: Export ONNX
        run: |
          python -m export.export_onnx \
            --config configs/train_small.yaml \
            --ckpt model/checkpoints \
            --out export/scraps.onnx \
            --seq_len 256 \
            --opset 17 \
            --check

      - name: Upload artifact (optional)
        uses: actions/upload-artifact@v4
        with:
          name: scraps-onnx
          path: |
            export/scraps.onnx
            tokenizer/bpe.json

      - name: Publish ONNX + tokenizer to HF model repo
        env:
          HF_TOKEN:      ${{ env.HF_TOKEN }}
          HF_MODEL_REPO: ${{ env.HF_MODEL_REPO }}
        run: |
          python - << 'PY'
          import os
          from huggingface_hub import HfApi, upload_file

          token   = os.environ["HF_TOKEN"]
          repo_id = os.environ["HF_MODEL_REPO"]

          api = HfApi()
          api.create_repo(repo_id, repo_type="model", exist_ok=True, private=False, token=token)

          for local, remote in [
              ("export/scraps.onnx", "export/scraps.onnx"),
              ("tokenizer/bpe.json", "tokenizer/bpe.json"),
          ]:
              print(f"⬆️ Uploading {local} -> {repo_id}:{remote}")
              upload_file(
                  path_or_fileobj=local,
                  path_in_repo=remote,
                  repo_id=repo_id,
                  repo_type="model",
                  token=token,
              )

          print("✅ Export + Upload complete!")
          PY
