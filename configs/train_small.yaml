# configs/train_rope_small.yaml
seed: 42
device: auto

vocab_path: tokenizer/bpe.json   # must match the tokenizer used for training

data:
  processed_dir: data/processed
  max_len: 256           
  train_batch_size: 64
  val_batch_size: 64
  num_workers: 0

model:
  d_model: 256            
  n_layers: 6
  n_heads: 4
  max_len: 256           
  dropout: 0.1               
  use_rope: true          
  tie_weights: true

optim:
  lr: 0.0003
  weight_decay: 0.01
  betas: [0.9, 0.95]
  grad_clip: 1.0
  epochs: 3
  warmup_steps: 1500
  scheduler: cosine
  mixed_precision: true
  grad_accum_steps: 2
