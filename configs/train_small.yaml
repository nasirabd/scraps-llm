# ===========================
# Scraps-LLM: test run
# ===========================
seed: 42
device: auto

vocab_path: tokenizer/bpe.json   # must match the tokenizer used for training

data:
  processed_dir: data/processed
  max_len: 256           
  train_batch_size: 512
  val_batch_size: 512
  num_workers: 2

model:
  d_model: 256            
  n_layers: 8
  n_heads: 8
  max_len: 256           
  dropout: 0.1               
  use_rope: true          
  tie_weights: true

optim:
  lr: 0.001
  weight_decay: 0.01
  betas: [0.9, 0.95]
  grad_clip: 1.0
  epochs: 3
  warmup_steps: auto
  scheduler: cosine
  mixed_precision: true
  grad_accum_steps: 1
